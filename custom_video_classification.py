# -*- coding: utf-8 -*-
"""custom_video_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZs36NSt7PDo42Ji9udODb937l6WzWHz
"""

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn’t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm() #LOOK FOR 0% IN LAST LINE
#!kill -9 -1

os.listdir()
data_dir='/content/gdrive/MyDrive/classifier_new/data'
proj_dir= '/content/gdrive/MyDrive/classifier_new'

os.listdir(proj_dir)

os.listdir(data_dir)

import keras
print('keras version ', keras.__version__)

import tensorflow as tf
print('tensorflow version ', tf.__version__)

import matplotlib
matplotlib.use('Agg')

#import the necessary packages
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.pooling import AveragePooling2D
from keras.applications import ResNet50
from keras.layers.core import Dropout, Flatten, Dense
from keras.layers import Input
from keras.models import Model
from keras.optimizers import SGD
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
#import argparse
import pickle
import cv2
import os

#initialise labels
LABELS = set(['news','sports','travel','music','entertainment'])
print(len(LABELS) , ' labels')

print('[INFO] loading images...')
imagePaths = list(paths.list_images(data_dir))
#14429 images total for 22 sports
print(len(imagePaths))

data=[]
labels=[]

for imagePath in imagePaths:
  #extract class label
  label = imagePath.split(os.path.sep)[-2]
  
  if label not in LABELS:
    continue
  
  #process image
  image = cv2.imread(imagePath)
  image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  image= cv2.resize(image,(224,224))
  
  data.append(image)
  labels.append(label)
  
#convert to array
data=np.array(data)
labels=np.array(labels)

#one-hot encode
lb= LabelBinarizer()
labels=lb.fit_transform(labels)

#train test split
(trainX, testX, trainY, testY)= train_test_split(data, labels,
                                                 test_size=0.25,
                                                stratify=labels,
                                                random_state=42)

#quick save
f=open(proj_dir + 'trained_model.pickle','wb')
f.write(pickle.dumps(lb))
f.close()

#initialise training data augmentation
trainAug=ImageDataGenerator(
    rotation_range=30,
    zoom_range=0.15,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    horizontal_flip=True,
    fill_mode='nearest')

#initialise val/test data aug (which we'll be adding mean subtraction to)
valAug=ImageDataGenerator()

#define ImageNet mean subtraction in RGB order
mean=np.array([123.68,116.779, 103.939], dtype='float32')
trainAug.mean=mean
valAug.mean=mean

#load ResNet-50 set head FC layers to off
baseModel=ResNet50(weights='imagenet', include_top=False,
                  input_tensor=Input(shape=(224,224,3)))

base=baseModel.output
base.shape

#construct head of model to be placed on base
headModel=baseModel.output
headModel=AveragePooling2D(pool_size=(7,7))(headModel)
headModel=Flatten(name='flatten')(headModel)
headModel=Dense(512, activation='relu')(headModel)
headModel=Dropout(0.5)(headModel)
headModel=Dense(len(lb.classes_), activation='softmax')(headModel)

#place head on base
model=Model(inputs=baseModel.input, outputs=headModel)

#loop over all layers i nbase model and freeze them so not updated
#during training process
for layer in baseModel.layers:
  layer.trainable=False

#compile
print('[INFO] compiling model...')
N = 50
#opt=SGD(lr=1e-4, momentum=0.9, decay=1e-4/args['epochs'])
opt=SGD(lr=1e-4, momentum=0.9, decay=1e-4/N)
model.compile(loss='categorical_crossentropy',
             optimizer=opt,
             metrics=['accuracy'])

#train head of network with few epochs
#allows new FC layers to start to become intialised with actual 'learned'
#values vs pure random

print('[INFO] training head...')
H=model.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),
                     steps_per_epoch=len(trainX)//32,
                     validation_data=valAug.flow(testX, testY),
                     validation_steps=len(testX)//32,
                     #epochs=args['epochs'])
                     epochs=N)

#evaluate
print('[INFO] evaluating network...')
predictions=model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1), 
                            predictions.argmax(axis=1),
                            target_names=lb.classes_))

#serialise model to disk
print('[INFO] serialising network...')
#model.save(args['model'])
'save entire model - arch + weights'
model.save(proj_dir + 'model/trained_model.model')

#https://jovianlin.io/saving-loading-keras-models/ recommends not using pickle
#to save keras models

#serialise label binarizer to disk
#f=open(args['label-bin'],'wb')
f=open(proj_dir + 'trained_model.pickle','wb')
f.write(pickle.dumps(lb))
f.close()

#This single HDF5 file will contain:
#the architecture of the model (allowing the recreation of the model)
#the weights of the model
#the training configuration (e.g. loss, optimizer)
#the state of the optimizer (allows you to resume the training from exactly where you left off)
#should be same as the .model file above

model.save('trained_model_arch_wts_22_50epochs.h5')

#only weights
model.save_weights(proj_dir + 'trained_model_weights_22_50epochs.h5')

#save only architecture
arch = proj_dir + 'model_arch_22sports.json'
arch = model.to_json()

with open('model_architecture_50epochs.json', 'w') as f:
    f.write(model.to_json())

# Model reconstruction from JSON file
with open('model_architecture_5epochs.json', 'r') as f:
    loaded_model = model_from_json(f.read())

loaded_model.load_weights('trained_model_weights_5epochs.h5')

#aim is to evaluate with laoded model and see if we get same results
#evaluate
print('[INFO] evaluating loaded network...')
predictions_with_loaded_model =loaded_model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1), 
                            predictions.argmax(axis=1),
                            target_names=lb.classes_))

#model reconstruction
print(proj_dir)
os.chdir(proj_dir)
from keras.models import model_from_json

with open('model_architecture_5epochs.json','r') as f:
  loaded_model=model_from_json(f.read())

loaded_model.load_weights('trained_model_weights_5epochs.h5')

loaded_model.summary()

from collections import deque
#We’ll use a deque  to implement our rolling prediction averaging.
#Our deque, Q , is initialized with a maxlen  equal to the args["size"] value 
from google.colab.patches import cv2_imshow
import numpy as np
import pickle
import cv2

#model == loaded_model
label_bin =  proj_dir + 'trained_model_50epochs.pickle'
input= proj_dir + 'examples/tennis.mp4'
size=1
#output_file= proj_dir + 'output/tennis_1frame_50epochs.avi'
output_file= proj_dir + 'output/tennis_1frame_50_epochs_smoothened.avi'

print(output_file)

#lb=pickle.loads(open(label_bin,'rb').read())

#intialise image mean for subtraction along with preds queue
mean=np.array([123.68,116.779,103.939][::1],dtype='float32')
size=1 #turns video classification into standard image classification
#size =128 #utilises prediction averaging algorithm
Q=deque(maxlen=size)

vs=cv2.VideoCapture(input)
writer=None
(W,H) = (None,None)

while True:
  #read next frame from file
  (grabbed, frame)=vs.read()
  
  #if frame was not grabbed then end of stream
  if not grabbed:
    break
    
  #if frame dims are empty, grab them
  if W is None or H is None:
    (H,W) = frame.shape[:2]
    
  #clone output frame, BGR to RGB, resize to 224,224 and mean subtract
  output = frame.copy()
  frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
  frame=cv2.resize(frame,(224,224)).astype('float32')
  frame -= mean
  
  #make predictions on frame and update predictions q
  #preds = loaded_model.predict(np.expand_dims(frame, axis=0))[0]
  preds = model.predict(np.expand_dims(frame, axis=0))[0]
  Q.append(preds)
  
  #perform prediction averaging over current history of previous preds
  results = np.array(Q).mean(axis=0)
  i=np.argmax(results)
  label=lb.classes_[i]
  
  #draw activity on output frame
  text ='activity: {}'.format(label)
  cv2.putText(output, text, (35,50), cv2.FONT_HERSHEY_SIMPLEX,
             1.25, (0,255,0),5)
  
  #check if video writer is none
  if writer is None:
    #initialise
    print('intialising writer')
    fourcc=cv2.VideoWriter_fourcc(*'MJPG')
    video_size = (W,H)
    writer = cv2.VideoWriter(output_file,fourcc,30, video_size)
  
  #write output frame to disk
  writer.write(output)
  
  #show output image
  #cv2.imshow('Output',output) this is NOT for colab
  #cv2_imshow(output) hashed to speed things up but it works on colab
  key=cv2.waitKey(1) & 0xFF
  
  #if 'q' key was pressed, break from loop
  if key==ord('q'):
    break
    
#release the file pointers
print('[INFO] cleaning up...')
writer.release()
vs.release()



